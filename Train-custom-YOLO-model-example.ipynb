{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8J2mugUzd02h"
      },
      "source": [
        "# Notebook to Train Custom YOLO Models\n",
        "\n",
        "This notebook is designed to help you train your own YOLO models from scratch or fine-tune existing ones.  \n",
        "You can use either:\n",
        "\n",
        "- **Local datasets** (e.g., in YOLO format stored on your Google Drive or GitHub)\n",
        "- **Datasets from Roboflow**, which can be easily imported via a download link\n",
        "- **Example dataset**, from a linked github repository\n",
        "\n",
        "The workflow includes:\n",
        "- Loading and organizing your dataset\n",
        "- Writing a custom `.yaml` config file\n",
        "- Launching training with the `ultralytics` YOLO implementation\n",
        "- (Optional) Exporting and evaluating your trained model\n",
        "\n",
        "This is ideal for training models on custom objects â€” whether you're working with animals, vehicles, tools, or underwater footage.\n",
        "\n",
        "---\n",
        "\n",
        "Make sure your dataset is in the correct YOLO structure:\n",
        "\n",
        "```\n",
        "dataset/\n",
        "â”œâ”€â”€ train/\n",
        "â”‚   â”œâ”€â”€ images/\n",
        "â”‚   â””â”€â”€ labels/\n",
        "â”œâ”€â”€ valid/\n",
        "â”‚   â”œâ”€â”€ images/\n",
        "â”‚   â””â”€â”€ labels/\n",
        "â”œâ”€â”€ test/   # optional\n",
        "â”‚   â”œâ”€â”€ images/\n",
        "â”‚   â””â”€â”€ labels/\n",
        "â””â”€â”€ data.yaml\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NaCMEgQja0X"
      },
      "source": [
        "# Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjpPg4mGKc1v"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import math\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "import numpy as np\n",
        "import time  # Import the time module\n",
        "from google.colab import runtime\n",
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "import platform\n",
        "import gdown\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "from __future__ import annotations\n",
        "import os, shutil, random, math\n",
        "from tempfile import mkdtemp\n",
        "from typing import Optional, Tuple, Dict, List, Sequence\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PhXOgsCiKfEd"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tdSMcABDNKW-"
      },
      "outputs": [],
      "source": [
        "# Pip install method (recommended)\n",
        "\n",
        "!pip install 'ultralytics'\n",
        "# !pip install ultralytics==8.3.195\n",
        "from IPython import display\n",
        "display.clear_output()\n",
        "\n",
        "import ultralytics\n",
        "ultralytics.checks()\n",
        "\n",
        "from ultralytics import YOLO\n",
        "from IPython.display import display, Image\n",
        "\n",
        "##tiling\n",
        "!pip install --upgrade git+https://github.com/Jordan-Pierce/yolo-tiling.git\n",
        "import sys\n",
        "sys.path.append('/content/yolo-tiling')\n",
        "\n",
        "from yolo_tiler import YoloTiler, TileConfig, TileProgress\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nfHNOsuNioW"
      },
      "outputs": [],
      "source": [
        "### Clone the folder with the functions\n",
        "!git clone https://github.com/albiangela/train-custom-YOLO-Colab.git\n",
        "import sys\n",
        "sys.path.append(\"/content/train-custom-YOLO-Colab/utils\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6-t6xVIad9qW"
      },
      "source": [
        "### ðŸ”— Connect to Your Google Drive\n",
        "\n",
        "Google Colab is a cloud-based Python environment that lets you run code in your browser, with free access to GPUs.  \n",
        "\n",
        "To access your datasets or save model outputs, youâ€™ll need to connect Colab to your Google Drive.\n",
        "This allows you to read and write files directly from your Drive, making it easier to store large datasets or export trained models.\n",
        "\n",
        "Run the cell below to authorize access."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CTOFyNjvvVw"
      },
      "outputs": [],
      "source": [
        "# Mount google drive\n",
        "drive.mount(\"/content/drive/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FsXsrUHkecwj"
      },
      "source": [
        "# Load data\n",
        "\n",
        "Use the interactive widget below to choose your dataset source. You can select one of:\n",
        "\n",
        "### 1. Roboflow\n",
        "- Paste your **Roboflow API key**, **workspace**, **project**, **version**, and **export format** (e.g., `yolov11`).\n",
        "- Tip: Usually copied from **Roboflow â†’ Export â†’ Show download code** on your project page.\n",
        "\n",
        "### 2. Google Drive link (shared `.zip`)\n",
        "- Copy the **Drive share URL** of your `.zip` (set sharing to **Anyone with the link**).\n",
        "- Paste it into the widget and confirm to download and extract.\n",
        "\n",
        "### 3. Example dataset from GitHub (Hexbugs)\n",
        "- Select **Hexbugs** to load a small, YOLO-formatted example dataset for quick testing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76MomXG7agbI"
      },
      "outputs": [],
      "source": [
        "# Select and fetch dataset via interactive UI\n",
        "\n",
        "\n",
        "# workspace_root = Path('/content/drive/MyDrive/Colab Notebooks')\n",
        "workspace_root = Path('/content/train-custom-YOLO-Colab/')\n",
        "if workspace_root.exists() and str(workspace_root) not in sys.path:\n",
        "    sys.path.append(str(workspace_root))\n",
        "if str(Path.cwd()) not in sys.path:\n",
        "    sys.path.append(str(Path.cwd()))\n",
        "\n",
        "from utils.datasets import launch_dataset_selector\n",
        "workspace_dataset_root = Path('/content/datasets')\n",
        "launch_dataset_selector(globals(), dataset_root=workspace_dataset_root)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1iqJ1D_mI2p"
      },
      "outputs": [],
      "source": [
        "# workspace_root = Path('/content/drive/MyDrive/Colab Notebooks')\n",
        "# if workspace_root.exists() and str(workspace_root) not in sys.path:\n",
        "#     sys.path.append(str(workspace_root))\n",
        "# if str(Path.cwd()) not in sys.path:\n",
        "#     sys.path.append(str(Path.cwd()))\n",
        "\n",
        "from utils.prep import (\n",
        "    Data,\n",
        "    auto_select_allowed_ids,\n",
        "    build_collapse_map,\n",
        "    build_new_class_ids_from_yaml,\n",
        "    check_dataset,\n",
        "    count_labels,\n",
        "    filter_labels,\n",
        "    make_data_yaml,\n",
        "    prepare_yolo_dataset,\n",
        "    simplify_labels,\n",
        "    summarize_classes,\n",
        "    tile_with_yolo_tiler,\n",
        ")\n",
        "\n",
        "dataset = Data('/content/datasets/', name)\n",
        "\n",
        "assert os.path.exists(dataset.location + name + '/train')\n",
        "\n",
        "dataset.location\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChgfP0gSqQXy"
      },
      "source": [
        "### Optional: Rename your annotated labels â€” What this cell does\n",
        "\n",
        "- **Purpose:** Collapse/rename your original classes into broader groups (e.g., many ray species â†’ `sting_ray`, many shark types â†’ `shark`) before training.\n",
        "\n",
        "- **Example**\n",
        "If you have a training dataset with 4 annotated species (e.g. 'cowtail_sting_ray','pink_stingray','blacktip_reef_shark','whitetip_reef_shark) but only want to train a simple model with two classes such as 'shark' and 'ray', then you can rename your classes to reduce the number of labels. In this case, depending on the class number that was assigned to your species on your annotations dataset (e.g. 0 is 'cowtail_sting_ray', 1 is 'pink_stingray',2 is 'blacktip_reef_shark',3 is 'whitetip_reef_shark), then you can remap as in the example below.\n",
        "\n",
        "- **`collapse_map`**  \n",
        "  Maps **original class IDs** â†’ **new group names**.  \n",
        "  - Only IDs listed here are **kept**.  \n",
        "  - Commented-out lines are **ignored** (those classes will be dropped).  \n",
        "\n",
        "\n",
        "- **`allowed_ids`**  \n",
        "  Set of original IDs youâ€™re keeping (i.e., the keys of `collapse_map`). Used to **filter** annotations.\n",
        "\n",
        "- **`new_class_ids`**  \n",
        "  Assigns **final numeric IDs** to the new groups (e.g., `shark:0`, `sting_ray:1`). These become your **contiguous class indices** used by YOLO.\n",
        "\n",
        "- **Outcome**  \n",
        "  After your relabel step runs, annotations are remapped so that all sharks share ID `0`, all sting rays share ID `1`, and unlisted classes are dropped.\n",
        "\n",
        "- **Donâ€™t forget**  \n",
        "  Update your `data.yaml` to match the **new class list and order** (e.g., `names: ['shark','sting_ray']`).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rSkIXggQgLC"
      },
      "outputs": [],
      "source": [
        "# ----------------------------------------------\n",
        "# ðŸ” Step 1: Collapse or remap class labels (optional)\n",
        "# ----------------------------------------------\n",
        "# OPTIONAL â€” Use this to change how labels are grouped, e.g. merging multiple shark types into one class.\n",
        "\n",
        "# Collapse original class IDs into broader categories\n",
        "collapse_map = {\n",
        "    0: 'sting_ray',    # cowtail_sting_ray\n",
        "    1: 'sting_ray',    # pink_Stingray\n",
        "    2: 'shark',        # blacktip_reef_shark\n",
        "    3: 'shark',        # whitetip_reef_shark\n",
        "\n",
        "}\n",
        "\n",
        "allowed_ids = set(collapse_map.keys())\n",
        "\n",
        "# Assign new numeric IDs to the collapsed categories\n",
        "new_class_ids = {\n",
        "    'shark': 0,\n",
        "    'sting_ray': 1,\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RQqDSauH1Zk"
      },
      "source": [
        "# Auto-select classes, remap IDs, and create a dataset\n",
        "\n",
        "## What this cell does\n",
        "- **Auto-selects** classes that meet minimum data thresholds.\n",
        "- **Builds** a contiguous ID mapping (old â†’ new).\n",
        "- **Prepares** a filtered, optionally rebalanced **train/val/test** split.\n",
        "\n",
        "## Steps\n",
        "\n",
        "### 1) Point to your dataset\n",
        "- `dataset_root = \"/content/datasets/\" + name` â€” the dataset folder chosen via the widget.\n",
        "\n",
        "### 2) Auto-select viable classes\n",
        "- `auto_select_allowed_ids(dataset_root, min_instances=40, min_files=10)` returns:\n",
        "  - `allowed_ids`: original class IDs that pass thresholds.\n",
        "  - `instance_counts`: total labeled objects per class.\n",
        "  - `file_counts`: images containing each class.\n",
        "- Tune thresholds to your data size:\n",
        "  - `min_instances=40` (min total objects per class).\n",
        "  - `min_files=10` (min images per class; set `None` to ignore).\n",
        "\n",
        "### 3) Build a compact remap\n",
        "- If `allowed_ids` is non-empty:\n",
        "  - `collapse_map = build_collapse_map(allowed_ids)` â†’ e.g. `{3:0, 4:1, 7:2}`.\n",
        "  - `new_class_ids = sorted(set(collapse_map.values()))` â†’ e.g. `[0, 1, 2]`.\n",
        "- Else: `collapse_map = None`, `new_class_ids = None` (skips remap).\n",
        "\n",
        "### 4) Prepare the filtered dataset\n",
        "- `prepare_yolo_dataset(...)` runs with:\n",
        "  - `out_dir = dataset_root + \"-filtered_split\"` â€” output folder.\n",
        "  - `do_change_labels=True` + `collapse_map/new_class_ids` â€” apply remap to contiguous IDs.\n",
        "  - `allowed_ids=...`, `drop_others=True` â€” keep only chosen classes; drop the rest.\n",
        "  - `prune_empty_fraction=0.9` â€” remove up to **90%** of empty images (keeps ~10% negatives).\n",
        "  - `do_tile=False` â€” skip tiling in this pass.\n",
        "  - `do_rebalance=True` â€” mitigate class imbalance across splits.\n",
        "  - `split=(0.7, 0.2, 0.1)` â€” train/val/test ratios.\n",
        "  - `remove_test=False` â€” keep a test split.\n",
        "\n",
        "## Output\n",
        "- A cleaned dataset at `...-filtered_split/` with:\n",
        "  - `train/`, `val/`, `test/` (images/labels),\n",
        "  - labels remapped to contiguous IDs,\n",
        "  - and an updated `data.yaml` compatible with YOLO.\n",
        "\n",
        "## Notes\n",
        "- If `allowed_ids` is empty, relax `min_instances`/`min_files` or inspect class distribution.\n",
        "- Keeping a small fraction of negatives (`prune_empty_fraction`) usually improves generalization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2MiyXWm1wlm-"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_root = \"/content/datasets/\" + name           # or before splitting\n",
        "\n",
        "allowed_ids, instance_counts, file_counts = auto_select_allowed_ids(\n",
        "    dataset_root,\n",
        "    min_instances=40,   # tune these to your dataset size\n",
        "    min_files=10        # optional; set None to ignore\n",
        ")\n",
        "\n",
        "# 2) Build the class remapping (old â†’ new contiguous ids)\n",
        "if allowed_ids:\n",
        "    collapse_map = build_collapse_map(allowed_ids)          # e.g. {3:0, 4:1, 7:2}\n",
        "    new_class_ids = sorted(set(collapse_map.values()))      # e.g. [0,1,2]\n",
        "else:\n",
        "    collapse_map, new_class_ids = None, None\n",
        "\n",
        "# 2) If you like the selection, run your prep with filtering only (no remap)\n",
        "if allowed_ids:\n",
        "    prepare_yolo_dataset(\n",
        "        dataset_path=dataset_root,\n",
        "        out_dir=dataset_root + \"-filtered_split\",\n",
        "        do_change_labels=True,\n",
        "        allowed_ids=allowed_ids,\n",
        "        collapse_map=collapse_map,      # â† now active\n",
        "        new_class_ids=new_class_ids,    # â† now active\n",
        "        drop_others=True,               # drop unwanted ids\n",
        "        prune_empty_fraction=0.9,\n",
        "        do_tile=False,\n",
        "        do_rebalance=True,\n",
        "        split=(0.7, 0.2, 0.1),\n",
        "        remove_test=False,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5Cw5zqzG6dl"
      },
      "source": [
        "### Check number of labels per class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZcBUSNjlV1n"
      },
      "outputs": [],
      "source": [
        "### Check here for each folder in the newsly created dataset '-filtered_split' how many labels per class we have\n",
        "out_dir = '/content/datasets/' + name + '-filtered_split'  # e.g., the same `out_dir` you passed to prepare_yolo_dataset\n",
        "check_dataset(out_dir)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "04Xc-HhZHJQq"
      },
      "source": [
        "## Optional: crete a new yaml file\n",
        "If you didn't change labels you can directly copy the yaml file from the original folder to the rebalanced_data folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pyCUZDznC1jz"
      },
      "outputs": [],
      "source": [
        "make_yaml = True      # set True to generate a new YAML\n",
        "copy_yaml = False     # keep False to avoid overwriting\n",
        "\n",
        "base_dir = \"/content/datasets/\"           # root folder\n",
        "\n",
        "src_yaml = os.path.join(base_dir, name, 'data.yaml')\n",
        "dst_yaml = os.path.join(out_dir, 'data.yaml')\n",
        "\n",
        "if make_yaml:\n",
        "    new_class_ids = build_new_class_ids_from_yaml(\n",
        "        src_yaml=src_yaml,\n",
        "        allowed_ids=allowed_ids,\n",
        "        collapse_map=collapse_map,\n",
        "    )\n",
        "    yaml_path = make_data_yaml(\n",
        "        dataset_root=out_dir,\n",
        "        new_class_ids=new_class_ids,\n",
        "        has_test=None,  # auto-detect from folder existence\n",
        "    )\n",
        "    print('âœ… data.yaml written to:', yaml_path)\n",
        "elif copy_yaml:\n",
        "    if os.path.exists(src_yaml):\n",
        "        shutil.copy2(src_yaml, dst_yaml)\n",
        "        print(f'âœ… Copied data.yaml from {src_yaml} â†’ {dst_yaml}')\n",
        "    else:\n",
        "        print(f'âŒ Source data.yaml not found at {src_yaml}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct9qNzOFPHz7"
      },
      "source": [
        "### Optional: Create a zip folder with the new dataset to dowload it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RKHvpSXMHJBZ"
      },
      "outputs": [],
      "source": [
        "make_zip = True                 # set to False to skip zipping\n",
        "\n",
        "# === PATHS ===\n",
        "folder = os.path.join(base_dir, f\"{name}-filtered_split\")\n",
        "zip_path = os.path.join(base_dir, f\"{name}-filtered_split.zip\")\n",
        "\n",
        "# === CONDITIONAL ZIP ===\n",
        "if make_zip:\n",
        "    if os.path.exists(folder):\n",
        "        if not os.path.exists(zip_path):\n",
        "            !zip -r -q \"{zip_path}\" \"{folder}\"\n",
        "            print(f\"âœ… Zipped: {zip_path}\")\n",
        "        else:\n",
        "            print(f\"âš ï¸ Zip file already exists: {zip_path}\")\n",
        "    else:\n",
        "        print(f\"âŒ Folder not found: {folder}\")\n",
        "else:\n",
        "    print(\"â­ï¸ Skipping ZIP creation (make_zip=False)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAa5KIz7Cik3"
      },
      "source": [
        "### Define output path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMy-FLWVwIG6"
      },
      "outputs": [],
      "source": [
        "### Change path to your folder\n",
        "REMOTE_URL = \"/content/drive/MyDrive/models/\" + name\n",
        "HOME = \"/content/datasets/\"\n",
        "\n",
        "# Change to HOME directory\n",
        "%cd {HOME}\n",
        "\n",
        "# Import os and create the folder if it doesn't exist\n",
        "import os\n",
        "\n",
        "if not os.path.exists(REMOTE_URL):\n",
        "    os.makedirs(REMOTE_URL)\n",
        "    print(f\"Directory '{REMOTE_URL}' created.\")\n",
        "else:\n",
        "    print(f\"Directory '{REMOTE_URL}' already exists.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dM5hr5hfPeV9"
      },
      "source": [
        "# Training: Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6xy69ImzmpvF"
      },
      "outputs": [],
      "source": [
        "# Change to home directory\n",
        "%cd {HOME}\n",
        "\n",
        "# ---- User-defined Settings ----\n",
        "resolution = 1080                # Image resolution for training\n",
        "epochs = 300                     # Number of training epochs\n",
        "batch_size = 6                   # Batch size\n",
        "base_model = \"yolo26s\"         # Choose model variant. Options: \"yolo11n-pose\", \"yolo11n-seg\", \"yolo11n\" etc.\n",
        "\n",
        "\n",
        "# ---- Auto-detect task type ----\n",
        "if \"-seg\" in base_model:\n",
        "    task = \"segment\"\n",
        "elif \"-pose\" in base_model:\n",
        "    task = \"pose\"\n",
        "else:\n",
        "    task = \"detect\"\n",
        "\n",
        "# ---- ðŸ”§ Training Settings ----\n",
        "common_settings = {\n",
        "    \"translate\": 0.05,       # Maximum image translation as data augmentation (in % of image size)\n",
        "    \"mixup\": 0.1,          # MixUp blending factor for image mixing (usually low for object detection)\n",
        "    \"copy_paste\": 0.3,      # Probability of using Copy-Paste augmentation (object pasting)\n",
        "    \"scale\": 0.3,           # Random scaling of images for augmentation\n",
        "    \"mosaic\": 0.5,             # Enable Mosaic augmentation (combines 4 images into 1)\n",
        "    \"close_mosaic\": 10,      # Number of epochs before disabling mosaic for better fine-tuning\n",
        "    \"line_width\": 1,         # Line width for label visualization\n",
        "    \"nms\": True,             # Apply Non-Maximum Suppression during inference\n",
        "    \"plots\": True,           # Save training plots (loss, mAP, etc.)\n",
        "    \"cache\": \"disk\",         # Caching mode: \"disk\" to speed up I/O\n",
        "    \"single_cls\": False,     # If True, treat all objects as one class (for class-agnostic detection)\n",
        "    \"amp\": True,             # Enable automatic mixed precision (reduces memory, speeds up training)\n",
        "    \"augment\": True,        # If True, applies augmentation at inference time\n",
        "    \"workers\": 16,            # Number of dataloader workers (adjust depending on your CPU)\n",
        "    \"multi_scale\": False,\n",
        "    \"hsv_h\": 0.015,\n",
        "    \"hsv_s\": 0.5,\n",
        "    \"hsv_v\": 0.4\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "# Modify task-specific augmentations\n",
        "if task == \"detect\":\n",
        "    common_settings.update({\n",
        "        \"degrees\": 10,       # Allow full rotation\n",
        "        \"flipud\": 0.0,       # Vertical flip probability\n",
        "        \"fliplr\": 0.0        # Horizontal flip probability\n",
        "    })\n",
        "else:\n",
        "    common_settings.update({\n",
        "        \"degrees\": 0,         # No rotation for pose/seg\n",
        "        \"flipud\": 0.0,\n",
        "        \"fliplr\": 0.0\n",
        "    })\n",
        "\n",
        "# Print CLI training parameters\n",
        "parms = \" \".join([f\"{k}={v}\" for k, v in common_settings.items()])\n",
        "print(\"ðŸ”§ Training params:\", parms)\n",
        "\n",
        "# ---- ðŸ—‚ï¸ Model Output Naming ----\n",
        "from datetime import datetime\n",
        "now = datetime.now()\n",
        "date_string = now.strftime(\"%Y-%m-%d-%H\") + \"_\" + dataset.name.replace(\" \", \"-\") + \"-\" + str(dataset.version)\n",
        "\n",
        "project = f\"{resolution}-{base_model}\"\n",
        "if common_settings[\"mosaic\"] > 0:\n",
        "    project += \"-mosaic\"\n",
        "\n",
        "# if sharkcam:\n",
        "#     project += \"-sharkcam\"  # Add logic if needed\n",
        "\n",
        "# ---- ðŸ§  Model Weights Source ----\n",
        "model = base_model  # or path to a pretrained model\n",
        "print(f\"ðŸ§ª resolution={resolution} | project={project} | date_string={date_string}\")\n",
        "print(f\"ðŸ“¦ model={model} | base_model={base_model} | task={task}\")\n",
        "\n",
        "# ---- ðŸ”’ Safety Check ----\n",
        "import os\n",
        "assert model == base_model or os.path.exists(model + \".pt\"), f\"Model path not found: {model}.pt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YUjFBKKqXa-u"
      },
      "source": [
        "# Training: Run Command"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D2YkphuiaE7_"
      },
      "outputs": [],
      "source": [
        "    # Change to your working directory\n",
        "%cd {HOME}\n",
        "\n",
        "# ---- Launch YOLO training ----\n",
        "yolo_cmd = f\"\"\"\n",
        "yolo task={task} \\\n",
        "     mode=train \\\n",
        "     resume=False \\\n",
        "     model={model}.pt \\\n",
        "     data={out_dir}/data.yaml \\\n",
        "     device=0 \\\n",
        "     name={date_string} \\\n",
        "     project={project} \\\n",
        "     epochs={epochs} \\\n",
        "     imgsz={resolution} \\\n",
        "     batch={batch_size} \\\n",
        "     patience=0 \\\n",
        "     visualize=True \\\n",
        "     {parms}\n",
        "\"\"\"\n",
        "\n",
        "# â–¶ï¸ Run the command\n",
        "!{yolo_cmd}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BMmQ2P0p6gH"
      },
      "source": [
        "#5) Locate last trained model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IIwmALvQnStL"
      },
      "outputs": [],
      "source": [
        "# Change to the working directory\n",
        "%cd {HOME}\n",
        "\n",
        "runs_root = f\"{dataset.location}/runs/{task}/{project}\"\n",
        "\n",
        "# List all subdirectories in the project folder\n",
        "all_subdirs = [os.path.join(runs_root, d) for d in os.listdir(runs_root)]\n",
        "all_subdirs = [d for d in all_subdirs if os.path.isdir(d)]\n",
        "\n",
        "# Keep only those that contain a trained model\n",
        "all_subdirs = [d for d in all_subdirs if os.path.exists(os.path.join(d, \"weights\", \"last.pt\"))]\n",
        "\n",
        "\n",
        "# Get the most recently modified subdirectory\n",
        "latest_subdir = max(all_subdirs, key=os.path.getmtime)\n",
        "\n",
        "# Construct the full path to the latest run\n",
        "# full_path = HOME + \"/\" + latest_subdir\n",
        "\n",
        "print(project)\n",
        "print(latest_subdir)\n",
        "# print(full_path)\n",
        "\n",
        "# Save training parameters to a parms.txt\n",
        "!echo \"{parms}\" > {latest_subdir}/parms.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iq8q97TnoXop"
      },
      "source": [
        "### Select and Save Best YOLO Model Based on mAP Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oWRI444BniY1"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Check if the best model weights file exists\n",
        "print(os.path.exists(os.path.join(latest_subdir, \"weights\", \"best.pt\")))\n",
        "\n",
        "# Load training results CSV\n",
        "csv = pd.read_csv(os.path.join(latest_subdir, \"results.csv\"))\n",
        "\n",
        "# Strip whitespace from column names\n",
        "csv.columns = [c.strip() for c in csv.columns]\n",
        "\n",
        "# Compute best epoch score (seg or det)\n",
        "if \"metrics/mAP50-95(M)\" in csv.columns:\n",
        "    combined = (csv[\"metrics/mAP50-95(M)\"] * 0.9 + csv[\"metrics/mAP50(M)\"] * 0.1) + \\\n",
        "               (csv[\"metrics/mAP50-95(B)\"] * 0.9 + csv[\"metrics/mAP50(B)\"] * 0.1)\n",
        "    index = combined.argmax()\n",
        "    best_map50_95 = float(csv[\"metrics/mAP50-95(M)\"].values[index])\n",
        "    best_map50 = float(csv[\"metrics/mAP50(M)\"].values[index])\n",
        "else:\n",
        "    combined = (csv[\"metrics/mAP50-95(B)\"] * 0.9 + csv[\"metrics/mAP50(B)\"] * 0.1)\n",
        "    index = combined.argmax()\n",
        "    best_map50_95 = float(csv[\"metrics/mAP50-95(B)\"].values[index])\n",
        "    best_map50 = float(csv[\"metrics/mAP50(B)\"].values[index])\n",
        "\n",
        "# Copy best.pt to /content with informative name\n",
        "from_path = os.path.join(latest_subdir, \"weights\", \"best.pt\")\n",
        "to_name = f\"{project}-{date_string}-mAP5095_{best_map50_95:.5f}-mAP50_{best_map50:.5f}.pt\"\n",
        "to_path = os.path.join(\"/content\", to_name)\n",
        "\n",
        "print(\"copying from\", from_path, \"to\", to_path)\n",
        "!cp \"{from_path}\" \"{to_path}\"\n",
        "!rsync --progress \"{to_path}\" \"{REMOTE_URL}/\"\n",
        "\n",
        "# Zip the whole run folder (latest_subdir) and upload\n",
        "run_name = os.path.basename(latest_subdir)  # e.g. 2026-01-31-11_panabat-1\n",
        "zip_path = os.path.join(\"/content\", f\"{project}-{run_name}.zip\")\n",
        "\n",
        "print(\"zipping\", latest_subdir, \"->\", zip_path)\n",
        "!zip -r \"{zip_path}\" \"{latest_subdir}\"\n",
        "!rsync --progress \"{zip_path}\" \"{REMOTE_URL}/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFKyygZYZsaW"
      },
      "outputs": [],
      "source": [
        "!rsync --progress {to_path} {REMOTE_URL}/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbj_y1WCpI0q"
      },
      "source": [
        "### Training results plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-urTWUkhRmn"
      },
      "outputs": [],
      "source": [
        "# Change working directory to HOME\n",
        "%cd {HOME}\n",
        "\n",
        "# Display the training results plot (e.g. loss and metrics curves)\n",
        "Image(filename=f'{latest_subdir}/results.png', width=1200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjYMah2GpMo4"
      },
      "source": [
        "### Sample batch of validation predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HI4nADCCj3F5"
      },
      "outputs": [],
      "source": [
        "# Change working directory to HOME\n",
        "%cd {HOME}\n",
        "\n",
        "# Display a sample batch of validation predictions (visual output of model)\n",
        "Image(filename=f'{latest_subdir}/val_batch0_pred.jpg', width=600)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ODk1VTlevxn"
      },
      "source": [
        "# 6) Validate Custom Model\n",
        "\n",
        "This step runs **model validation** using the best trained checkpoint (`best.pt`) on the validation dataset defined in `data.yaml`. It evaluates the model's performance using standard YOLO metrics, such as:\n",
        "\n",
        "- **mAP50**: mean Average Precision at IoU threshold 0.5\n",
        "- **mAP50-95**: mean AP across IoU thresholds from 0.5 to 0.95\n",
        "- **Precision & Recall** for each class\n",
        "\n",
        "The validation results will be saved inside the specified project folder and include:\n",
        "\n",
        "- A `results.png` file with training/validation curves\n",
        "- A `confusion_matrix.png` for classification performance\n",
        "- A `val_batch0_pred.jpg` showing predicted bounding boxes on a sample batch\n",
        "\n",
        "You can use these visual and quantitative outputs to assess if the model generalizes well to unseen data. [link text](https://)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpyuwrNlXc1P"
      },
      "outputs": [],
      "source": [
        "# Change working directory to HOME\n",
        "%cd {HOME}\n",
        "\n",
        "# Display the training results plot (e.g. loss and metrics curves)\n",
        "Image(filename=f'{latest_subdir}/results.png', width=1200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i4eASbcWkQBq"
      },
      "source": [
        "# 7) Run Inference on Validation Images\n",
        "\n",
        "This step performs **inference (prediction)** using the best trained YOLO model (`best.pt`) on the validation image set. It is useful to **visually inspect how the model performs** on real images after training.\n",
        "\n",
        "What this does:\n",
        "\n",
        "- Removes any existing `predict` folder to avoid clutter or overwriting previous predictions\n",
        "- Runs YOLO in `predict` mode using:\n",
        "  - The best model checkpoint\n",
        "  - Images from the validation set\n",
        "  - A low confidence threshold (`conf=0.1`) to allow more predictions for visual inspection\n",
        "  - The specified image size (`imgsz`)\n",
        "- Saves predicted images (with boxes, masks, or keypoints depending on the task) in a new folder under the project directory: `runs/predict`\n",
        "\n",
        "This is especially helpful for qualitatively checking the model's detection performance, spotting failure cases, or selecting images for visualization or presentations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wjc1ctZykYuf"
      },
      "outputs": [],
      "source": [
        "%cd {HOME}\n",
        "\n",
        "best_ckpt = os.path.join(latest_subdir, \"weights\", \"best.pt\")\n",
        "data_yaml = os.path.join(out_dir, \"data.yaml\")   # <-- fix: use out_dir\n",
        "\n",
        "assert os.path.exists(best_ckpt), best_ckpt\n",
        "assert os.path.exists(data_yaml), data_yaml\n",
        "\n",
        "!yolo task={task} mode=val model=\"{best_ckpt}\" data=\"{data_yaml}\" project=\"{project}\" imgsz={resolution} line_width=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vksXiQIBp7pJ"
      },
      "source": [
        "### Zip and Save Prediction Results\n",
        "\n",
        "This step creates a ZIP archive of the prediction results generated in the previous step. The archive is saved in your home directory and named using the training subdirectory name (to make it easy to track which model it came from).\n",
        "\n",
        "This makes it simple to download, share, or upload the predictions for external use (e.g., for presentations, manual inspection, or further analysis)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSrUCvQS7wpK"
      },
      "outputs": [],
      "source": [
        "# Change working directory to HOME\n",
        "%cd {HOME}\n",
        "\n",
        "best_ckpt   = os.path.join(latest_subdir, \"weights\", \"best.pt\")\n",
        "valid_imgs  = os.path.join(out_dir, \"valid\", \"images\")   # <-- use out_dir (your dataset root)\n",
        "pred_dir    = os.path.join(project, \"predict\")           # project/name\n",
        "\n",
        "print(\"Model:\", best_ckpt)\n",
        "print(\"Source:\", valid_imgs)\n",
        "print(\"Will write to:\", pred_dir)\n",
        "\n",
        "# Remove any previous YOLO prediction results (safe delete)\n",
        "if os.path.isdir(pred_dir):\n",
        "    shutil.rmtree(pred_dir)\n",
        "\n",
        "# Run YOLO prediction on validation images using the best model checkpoint\n",
        "!yolo task={task} mode=predict model=\"{best_ckpt}\" project=\"{project}\" name=\"predict\" conf=0.1 source=\"{valid_imgs}\" save=True imgsz={resolution} line_width=1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0znAqJQw77sc"
      },
      "source": [
        "# 10) Display Sample Predictions\n",
        "\n",
        "This step randomly selects and displays 5 predicted images from the `predict` folder.\n",
        "\n",
        "Each image includes the model's output (e.g., bounding boxes, masks, or keypoints) overlaid on the validation images.  \n",
        "It provides a quick **visual inspection** of model performance across different examples.  \n",
        "\n",
        "This qualitative check helps identify:\n",
        "- How well the model localizes objects\n",
        "- Possible false positives or negatives\n",
        "- Class confusion or missed detections"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jbVjEtPAkz3j"
      },
      "outputs": [],
      "source": [
        "# Find newest predict* folder for this task/project\n",
        "pred_root = f\"/content/datasets/runs/{task}/{project}\"\n",
        "pred_candidates = [p for p in glob.glob(f\"{pred_root}/predict*\") if os.path.isdir(p)]\n",
        "assert pred_candidates, f\"No predict folder found in: {pred_root}\"\n",
        "pred_dir = max(pred_candidates, key=os.path.getmtime)\n",
        "\n",
        "# Randomly select up to 5 predicted images\n",
        "imgs = glob.glob(f\"{pred_dir}/*.jpg\")\n",
        "assert imgs, f\"No .jpg files found in: {pred_dir}\"\n",
        "\n",
        "k = min(5, len(imgs))\n",
        "files = np.random.choice(imgs, size=k, replace=False)\n",
        "print(files.shape)\n",
        "print(\"Using:\", pred_dir)\n",
        "\n",
        "for image_path in files:\n",
        "    display(Image(filename=image_path, height=600))\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4x6ansX2Yyd"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Wait for 30 seconds (e.g., to ensure all background tasks finish before disconnecting)\n",
        "time.sleep(30)\n",
        "\n",
        "# Gracefully disconnect the current Colab runtime session\n",
        "runtime.unassign()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ovQgOj_xSNDg"
      },
      "source": [
        "## ðŸ† Congratulations\n",
        "\n",
        "### Find more learning resources here\n",
        "\n",
        "Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n",
        "\n",
        "- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n",
        "- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n",
        "- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n",
        "- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n",
        "\n",
        "### Convert data formats\n",
        "\n",
        "Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n",
        "\n",
        "### Connect computer vision to your project logic\n",
        "\n",
        "[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BDXH0Pin6_p1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}