{"cells":[{"cell_type":"markdown","metadata":{"id":"8J2mugUzd02h"},"source":["# Notebook to Train Custom YOLO Models\n","\n","This notebook is designed to help you train your own YOLO models from scratch or fine-tune existing ones.  \n","You can use either:\n","\n","- **Local datasets** (e.g., in YOLO format stored on your Google Drive or GitHub)\n","- **Datasets from Roboflow**, which can be easily imported via a download link\n","- **Example dataset**, from a linked github repository\n","\n","The workflow includes:\n","- Loading and organizing your dataset\n","- Writing a custom `.yaml` config file\n","- Launching training with the `ultralytics` YOLO implementation\n","- (Optional) Exporting and evaluating your trained model\n","\n","This is ideal for training models on custom objects ‚Äî whether you're working with animals, vehicles, tools, or underwater footage.\n","\n","---\n","\n","Make sure your dataset is in the correct YOLO structure:\n","\n","```\n","dataset/\n","‚îú‚îÄ‚îÄ train/\n","‚îÇ   ‚îú‚îÄ‚îÄ images/\n","‚îÇ   ‚îî‚îÄ‚îÄ labels/\n","‚îú‚îÄ‚îÄ valid/\n","‚îÇ   ‚îú‚îÄ‚îÄ images/\n","‚îÇ   ‚îî‚îÄ‚îÄ labels/\n","‚îú‚îÄ‚îÄ test/   # optional\n","‚îÇ   ‚îú‚îÄ‚îÄ images/\n","‚îÇ   ‚îî‚îÄ‚îÄ labels/\n","‚îî‚îÄ‚îÄ data.yaml\n","```"]},{"cell_type":"markdown","metadata":{"id":"4NaCMEgQja0X"},"source":["# Import libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"CjpPg4mGKc1v","executionInfo":{"status":"ok","timestamp":1763806426450,"user_tz":-60,"elapsed":477,"user":{"displayName":"Angela Albi","userId":"15664309201935832859"}}},"outputs":[],"source":["import os\n","import random\n","import shutil\n","import math\n","import glob\n","from IPython.display import Image, display\n","import numpy as np\n","import time  # Import the time module\n","from google.colab import runtime\n","from google.colab import drive\n","from pathlib import Path\n","import zipfile\n","import platform\n","import gdown\n","import sys\n","from pathlib import Path\n","import sys\n","\n","from __future__ import annotations\n","import os, shutil, random, math\n","from tempfile import mkdtemp\n","from typing import Optional, Tuple, Dict, List, Sequence\n","\n","from collections import Counter, defaultdict\n","\n"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"PhXOgsCiKfEd","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763806426617,"user_tz":-60,"elapsed":156,"user":{"displayName":"Angela Albi","userId":"15664309201935832859"}},"outputId":"30428ccb-3041-4599-e1b0-079a97cd83b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Sat Nov 22 10:13:47 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   41C    P0             49W /  400W |       0MiB /  40960MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}],"source":["!nvidia-smi"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"tdSMcABDNKW-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763806452391,"user_tz":-60,"elapsed":25760,"user":{"displayName":"Angela Albi","userId":"15664309201935832859"}},"outputId":"196fb269-ded2-44e3-b7b6-fbd2b6130027"},"outputs":[{"output_type":"stream","name":"stdout","text":["Ultralytics 8.3.230 üöÄ Python-3.12.12 torch-2.9.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n","Setup complete ‚úÖ (12 CPUs, 83.5 GB RAM, 37.9/235.7 GB disk)\n","Collecting git+https://github.com/Jordan-Pierce/yolo-tiling.git\n","  Cloning https://github.com/Jordan-Pierce/yolo-tiling.git to /tmp/pip-req-build-hd0imb2f\n","  Running command git clone --filter=blob:none --quiet https://github.com/Jordan-Pierce/yolo-tiling.git /tmp/pip-req-build-hd0imb2f\n","  Resolved https://github.com/Jordan-Pierce/yolo-tiling.git to commit 7808dc761b6e064dc4963025a719eb095827b656\n","  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from yolo-tiling==0.0.27) (2.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from yolo-tiling==0.0.27) (4.67.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from yolo-tiling==0.0.27) (2.2.2)\n","Requirement already satisfied: Shapely in /usr/local/lib/python3.12/dist-packages (from yolo-tiling==0.0.27) (2.1.2)\n","Collecting rasterio (from yolo-tiling==0.0.27)\n","  Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from yolo-tiling==0.0.27) (4.12.0.88)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from yolo-tiling==0.0.27) (3.10.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from yolo-tiling==0.0.27) (6.0.3)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->yolo-tiling==0.0.27) (1.3.3)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->yolo-tiling==0.0.27) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->yolo-tiling==0.0.27) (4.60.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->yolo-tiling==0.0.27) (1.4.9)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->yolo-tiling==0.0.27) (25.0)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->yolo-tiling==0.0.27) (11.3.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->yolo-tiling==0.0.27) (3.2.5)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->yolo-tiling==0.0.27) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->yolo-tiling==0.0.27) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->yolo-tiling==0.0.27) (2025.2)\n","Collecting affine (from rasterio->yolo-tiling==0.0.27)\n","  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.12/dist-packages (from rasterio->yolo-tiling==0.0.27) (25.4.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from rasterio->yolo-tiling==0.0.27) (2025.11.12)\n","Requirement already satisfied: click>=4.0 in /usr/local/lib/python3.12/dist-packages (from rasterio->yolo-tiling==0.0.27) (8.3.1)\n","Collecting cligj>=0.5 (from rasterio->yolo-tiling==0.0.27)\n","  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n","Collecting click-plugins (from rasterio->yolo-tiling==0.0.27)\n","  Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl.metadata (6.5 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->yolo-tiling==0.0.27) (1.17.0)\n","Downloading rasterio-1.4.3-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.3 MB)\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n","Downloading affine-2.4.0-py3-none-any.whl (15 kB)\n","Downloading click_plugins-1.1.1.2-py2.py3-none-any.whl (11 kB)\n","Building wheels for collected packages: yolo-tiling\n","  Building wheel for yolo-tiling (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for yolo-tiling: filename=yolo_tiling-0.0.27-py2.py3-none-any.whl size=24308 sha256=f423099480b4152162f68b5fefe10cf51c501c2637bc392bf63d63deea7538b7\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-75fyu7_r/wheels/af/95/0b/a13dac19fefc00c59953b9c2daaf5ba33d64cf6df02355210e\n","Successfully built yolo-tiling\n","Installing collected packages: cligj, click-plugins, affine, rasterio, yolo-tiling\n","Successfully installed affine-2.4.0 click-plugins-1.1.1.2 cligj-0.7.2 rasterio-1.4.3 yolo-tiling-0.0.27\n"]}],"source":["# Pip install method (recommended)\n","\n","!pip install 'ultralytics'\n","# !pip install ultralytics==8.3.195\n","from IPython import display\n","display.clear_output()\n","\n","import ultralytics\n","ultralytics.checks()\n","\n","from ultralytics import YOLO\n","from IPython.display import display, Image\n","\n","##tiling\n","!pip install --upgrade git+https://github.com/Jordan-Pierce/yolo-tiling.git\n","import sys\n","sys.path.append('/content/yolo-tiling')\n","\n","from yolo_tiler import YoloTiler, TileConfig, TileProgress\n"]},{"cell_type":"markdown","metadata":{"id":"6-t6xVIad9qW"},"source":["### üîó Connect to Your Google Drive\n","\n","Google Colab is a cloud-based Python environment that lets you run code in your browser, with free access to GPUs.  \n","\n","To access your datasets or save model outputs, you‚Äôll need to connect Colab to your Google Drive.\n","This allows you to read and write files directly from your Drive, making it easier to store large datasets or export trained models.\n","\n","Run the cell below to authorize access."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"9CTOFyNjvvVw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1763806492022,"user_tz":-60,"elapsed":39627,"user":{"displayName":"Angela Albi","userId":"15664309201935832859"}},"outputId":"c91947a7-ceb3-45ed-d2a2-36cfc796cb5c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["# Mount google drive\n","drive.mount(\"/content/drive/\")"]},{"cell_type":"markdown","metadata":{"id":"FsXsrUHkecwj"},"source":["# Load data\n","\n","Use the interactive widget below to choose your dataset source. You can select one of:\n","\n","### 1. Roboflow\n","- Paste your **Roboflow API key**, **workspace**, **project**, **version**, and **export format** (e.g., `yolov11`).\n","- Tip: Usually copied from **Roboflow ‚Üí Export ‚Üí Show download code** on your project page.\n","\n","### 2. Google Drive link (shared `.zip`)\n","- Copy the **Drive share URL** of your `.zip` (set sharing to **Anyone with the link**).\n","- Paste it into the widget and confirm to download and extract.\n","\n","### 3. Example dataset from GitHub (Hexbugs)\n","- Select **Hexbugs** to load a small, YOLO-formatted example dataset for quick testing."]},{"cell_type":"code","source":[],"metadata":{"id":"v4FYyR9tD5Dh"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"76MomXG7agbI","colab":{"base_uri":"https://localhost:8080/","height":350,"referenced_widgets":["d17beb7197e343719c1dd849c8bccf9d","5f86d560cb3b4e73aa4ba34ae62dfe29","4ef1d6b0299445b79c99bcd9b60da725","8f782975d4ff4c759bc6377ac3d51b2b","974842e0ad2c474d8815776f27aae193","6cf06b586d6445989afe76aa0f8e6629","46c3983db70e481b9798000a583e5fc6","a8db64c0155f4a1a98175d0ab7a7cbab","6ac4fe28b6b544cb8c69dcd6f4110a67","05c134ced70444899ce786ec7f2c1505","70445951fbaa49a38fc1b7c3515e8cd1","f28898bf3fd940d785ccc2f3cec492cf","cea5eb5ca4ed44568e9f6027994bccc9","5cc9056bcf594ccb8b2e28e44e872627","5bfe88b3871d4fe4a6e2f8fd5a071b0d","79861276c5b340289c2d9a20f85bda15","aef7e6cb6c2a4e9ba33008767485870e","86368bf2e52a479bb18fd5f0b668e0d3","1367db7f2eb44dba9de71a5b004085ec","8dbfe0d25cfc45478b21d391ef42dd34","7311b0c2bffc4ba485716857b97041e8","107b3677b0d0480a96d367f892a70ab2","49f1c1507e194fa587a3359a3eb878d1","6788544e0fdd4ad7b51f0fe6770622f9","883074ceb2af46428b058128e2ba9346","bd462e3e0a5b410d9a72ea520ba7f6fc","e544351928b6479ea28f976f52b819f3","063d9dc5ec064636bbd762d374c100b9","d806564cd6b648e9b36711267e96c353","89aaf2288271422699438d019dea1dab","9424484fef9b441f8c99c62a5487c42b"]},"executionInfo":{"status":"ok","timestamp":1763806559200,"user_tz":-60,"elapsed":98,"user":{"displayName":"Angela Albi","userId":"15664309201935832859"}},"outputId":"f129adcb-d8cb-443f-bbcd-4c4fb5c866b3"},"outputs":[{"output_type":"display_data","data":{"text/plain":["VBox(children=(Dropdown(description='Source:', options=(('Roboflow snippet', 'roboflow'), ('Google Drive link'‚Ä¶"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d17beb7197e343719c1dd849c8bccf9d"}},"metadata":{}}],"source":["# Select and fetch dataset via interactive UI\n","\n","\n","workspace_root = Path('/content/drive/MyDrive/Colab Notebooks')\n","if workspace_root.exists() and str(workspace_root) not in sys.path:\n","    sys.path.append(str(workspace_root))\n","if str(Path.cwd()) not in sys.path:\n","    sys.path.append(str(Path.cwd()))\n","\n","from train_custom_yolo import launch_dataset_selector\n","workspace_dataset_root = Path('/content/datasets')\n","launch_dataset_selector(globals(), dataset_root=workspace_dataset_root)\n","\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"B1iqJ1D_mI2p","colab":{"base_uri":"https://localhost:8080/","height":211},"executionInfo":{"status":"error","timestamp":1763806497824,"user_tz":-60,"elapsed":15,"user":{"displayName":"Angela Albi","userId":"15664309201935832859"}},"outputId":"b601ff7b-39a5-425f-c440-21cac80f5497"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'name' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-4086226798.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/datasets/'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'/train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'name' is not defined"]}],"source":["workspace_root = Path('/content/drive/MyDrive/Colab Notebooks')\n","if workspace_root.exists() and str(workspace_root) not in sys.path:\n","    sys.path.append(str(workspace_root))\n","if str(Path.cwd()) not in sys.path:\n","    sys.path.append(str(Path.cwd()))\n","\n","from train_custom_yolo import (\n","    Data,\n","    auto_select_allowed_ids,\n","    build_collapse_map,\n","    build_new_class_ids_from_yaml,\n","    check_dataset,\n","    count_labels,\n","    filter_labels,\n","    make_data_yaml,\n","    prepare_yolo_dataset,\n","    simplify_labels,\n","    summarize_classes,\n","    tile_with_yolo_tiler,\n",")\n","\n","dataset = Data('/content/datasets/', name)\n","\n","assert os.path.exists(dataset.location + name + '/train')\n","\n","dataset.location\n"]},{"cell_type":"markdown","metadata":{"id":"ChgfP0gSqQXy"},"source":["### Optional: Rename your annotated labels ‚Äî What this cell does\n","\n","- **Purpose:** Collapse/rename your original classes into broader groups (e.g., many ray species ‚Üí `sting_ray`, many shark types ‚Üí `shark`) before training.\n","\n","- **Example**\n","If you have a training dataset with 4 annotated species (e.g. 'cowtail_sting_ray','pink_stingray','blacktip_reef_shark','whitetip_reef_shark) but only want to train a simple model with two classes such as 'shark' and 'ray', then you can rename your classes to reduce the number of labels. In this case, depending on the class number that was assigned to your species on your annotations dataset (e.g. 0 is 'cowtail_sting_ray', 1 is 'pink_stingray',2 is 'blacktip_reef_shark',3 is 'whitetip_reef_shark), then you can remap as in the example below.\n","\n","- **`collapse_map`**  \n","  Maps **original class IDs** ‚Üí **new group names**.  \n","  - Only IDs listed here are **kept**.  \n","  - Commented-out lines are **ignored** (those classes will be dropped).  \n","\n","\n","- **`allowed_ids`**  \n","  Set of original IDs you‚Äôre keeping (i.e., the keys of `collapse_map`). Used to **filter** annotations.\n","\n","- **`new_class_ids`**  \n","  Assigns **final numeric IDs** to the new groups (e.g., `shark:0`, `sting_ray:1`). These become your **contiguous class indices** used by YOLO.\n","\n","- **Outcome**  \n","  After your relabel step runs, annotations are remapped so that all sharks share ID `0`, all sting rays share ID `1`, and unlisted classes are dropped.\n","\n","- **Don‚Äôt forget**  \n","  Update your `data.yaml` to match the **new class list and order** (e.g., `names: ['shark','sting_ray']`).\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6rSkIXggQgLC"},"outputs":[],"source":["# ----------------------------------------------\n","# üîÅ Step 1: Collapse or remap class labels (optional)\n","# ----------------------------------------------\n","# OPTIONAL ‚Äî Use this to change how labels are grouped, e.g. merging multiple shark types into one class.\n","\n","# Collapse original class IDs into broader categories\n","collapse_map = {\n","    0: 'sting_ray',    # cowtail_sting_ray\n","    1: 'sting_ray',    # pink_Stingray\n","    2: 'shark',        # blacktip_reef_shark\n","    3: 'shark',        # whitetip_reef_shark\n","\n","}\n","\n","allowed_ids = set(collapse_map.keys())\n","\n","# Assign new numeric IDs to the collapsed categories\n","new_class_ids = {\n","    'shark': 0,\n","    'sting_ray': 1,\n","}"]},{"cell_type":"markdown","source":["# Auto-select classes, remap IDs, and create a dataset\n","\n","## What this cell does\n","- **Auto-selects** classes that meet minimum data thresholds.\n","- **Builds** a contiguous ID mapping (old ‚Üí new).\n","- **Prepares** a filtered, optionally rebalanced **train/val/test** split.\n","\n","## Steps\n","\n","### 1) Point to your dataset\n","- `dataset_root = \"/content/datasets/\" + name` ‚Äî the dataset folder chosen via the widget.\n","\n","### 2) Auto-select viable classes\n","- `auto_select_allowed_ids(dataset_root, min_instances=40, min_files=10)` returns:\n","  - `allowed_ids`: original class IDs that pass thresholds.\n","  - `instance_counts`: total labeled objects per class.\n","  - `file_counts`: images containing each class.\n","- Tune thresholds to your data size:\n","  - `min_instances=40` (min total objects per class).\n","  - `min_files=10` (min images per class; set `None` to ignore).\n","\n","### 3) Build a compact remap\n","- If `allowed_ids` is non-empty:\n","  - `collapse_map = build_collapse_map(allowed_ids)` ‚Üí e.g. `{3:0, 4:1, 7:2}`.\n","  - `new_class_ids = sorted(set(collapse_map.values()))` ‚Üí e.g. `[0, 1, 2]`.\n","- Else: `collapse_map = None`, `new_class_ids = None` (skips remap).\n","\n","### 4) Prepare the filtered dataset\n","- `prepare_yolo_dataset(...)` runs with:\n","  - `out_dir = dataset_root + \"-filtered_split\"` ‚Äî output folder.\n","  - `do_change_labels=True` + `collapse_map/new_class_ids` ‚Äî apply remap to contiguous IDs.\n","  - `allowed_ids=...`, `drop_others=True` ‚Äî keep only chosen classes; drop the rest.\n","  - `prune_empty_fraction=0.9` ‚Äî remove up to **90%** of empty images (keeps ~10% negatives).\n","  - `do_tile=False` ‚Äî skip tiling in this pass.\n","  - `do_rebalance=True` ‚Äî mitigate class imbalance across splits.\n","  - `split=(0.7, 0.2, 0.1)` ‚Äî train/val/test ratios.\n","  - `remove_test=False` ‚Äî keep a test split.\n","\n","## Output\n","- A cleaned dataset at `...-filtered_split/` with:\n","  - `train/`, `val/`, `test/` (images/labels),\n","  - labels remapped to contiguous IDs,\n","  - and an updated `data.yaml` compatible with YOLO.\n","\n","## Notes\n","- If `allowed_ids` is empty, relax `min_instances`/`min_files` or inspect class distribution.\n","- Keeping a small fraction of negatives (`prune_empty_fraction`) usually improves generalization."],"metadata":{"id":"-RQqDSauH1Zk"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"2MiyXWm1wlm-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761139319947,"user_tz":-120,"elapsed":55,"user":{"displayName":"Angela Albi","userId":"15664309201935832859"}},"outputId":"c0449e86-3ec2-4f46-b593-87caeeceb127"},"outputs":[{"output_type":"stream","name":"stdout","text":["Dataset scanned: /content/datasets/hexbugs\n","Label files: 36  (empty: 0)\n","\n","Per-class summary:\n","class_id | instances | files_present_in\n","      0 |       180 |              36\n","\n","Selection thresholds: min_instances >= 40 and min_files >= 10\n","‚Üí allowed_ids = [0]\n","üìù Working directory: /tmp/yolo_prep_9bm3gf_r\n","üîç Filtering labels ‚Ä¶\n","   ‚Ä¢ filter_labels: kept=180, dropped=0, emptied_files=0\n","üìë Collapsing class taxonomy ‚Ä¶\n","   ‚Ä¢ simplify_labels: remapped=180 kept_as_is=0 dropped=0\n","   ‚Ä¢ prune_empty_labels: removed 0 empty label/image pairs\n","üîÄ Rebalancing (splitting) into final out_dir ‚Ä¶\n","   ‚Ä¢ split counts: train=25 valid=7 test=4\n","   ‚Üí wrote splits to: /content/datasets/hexbugs-filtered_split\n","‚úÖ Final dataset written to: /content/datasets/hexbugs-filtered_split\n"]}],"source":["\n","dataset_root = \"/content/datasets/\" + name           # or before splitting\n","\n","allowed_ids, instance_counts, file_counts = auto_select_allowed_ids(\n","    dataset_root,\n","    min_instances=40,   # tune these to your dataset size\n","    min_files=10        # optional; set None to ignore\n",")\n","\n","# 2) Build the class remapping (old ‚Üí new contiguous ids)\n","if allowed_ids:\n","    collapse_map = build_collapse_map(allowed_ids)          # e.g. {3:0, 4:1, 7:2}\n","    new_class_ids = sorted(set(collapse_map.values()))      # e.g. [0,1,2]\n","else:\n","    collapse_map, new_class_ids = None, None\n","\n","# 2) If you like the selection, run your prep with filtering only (no remap)\n","if allowed_ids:\n","    prepare_yolo_dataset(\n","        dataset_path=dataset_root,\n","        out_dir=dataset_root + \"-filtered_split\",\n","        do_change_labels=True,\n","        allowed_ids=allowed_ids,\n","        collapse_map=collapse_map,      # ‚Üê now active\n","        new_class_ids=new_class_ids,    # ‚Üê now active\n","        drop_others=True,               # drop unwanted ids\n","        prune_empty_fraction=0.9,\n","        do_tile=False,\n","        do_rebalance=True,\n","        split=(0.7, 0.2, 0.1),\n","        remove_test=False,\n","    )"]},{"cell_type":"markdown","metadata":{"id":"W5Cw5zqzG6dl"},"source":["### Check number of labels per class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nZcBUSNjlV1n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761139322214,"user_tz":-120,"elapsed":8,"user":{"displayName":"Angela Albi","userId":"15664309201935832859"}},"outputId":"9df11c20-cf9b-4f95-803d-210a3a06cd26"},"outputs":[{"output_type":"stream","name":"stdout","text":["Checking split dataset at: /content/datasets/hexbugs-filtered_split\n","\n","[train] labels: /content/datasets/hexbugs-filtered_split/train/labels\n","Class counts: {0: 125}\n","Empty labels: 0 / 25\n","\n","[valid] labels: /content/datasets/hexbugs-filtered_split/valid/labels\n","Class counts: {0: 35}\n","Empty labels: 0 / 7\n","\n","[test] labels: /content/datasets/hexbugs-filtered_split/test/labels\n","Class counts: {0: 20}\n","Empty labels: 0 / 4\n"]}],"source":["### Check here for each folder in the newsly created dataset '-filtered_split' how many labels per class we have\n","out_dir = '/content/datasets/' + name + '-filtered_split'  # e.g., the same `out_dir` you passed to prepare_yolo_dataset\n","check_dataset(out_dir)\n"]},{"cell_type":"markdown","metadata":{"id":"04Xc-HhZHJQq"},"source":["## Optional: crete a new yaml file\n","If you didn't change labels you can directly copy the yaml file from the original folder to the rebalanced_data folder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pyCUZDznC1jz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761139325669,"user_tz":-120,"elapsed":16,"user":{"displayName":"Angela Albi","userId":"15664309201935832859"}},"outputId":"e73ca49a-5e8b-4ff4-ed9c-0718e9cdc588"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ data.yaml written to: /content/datasets/hexbugs-filtered_split/data.yaml\n"]}],"source":["make_yaml = True      # set True to generate a new YAML\n","copy_yaml = False     # keep False to avoid overwriting\n","\n","base_dir = \"/content/datasets/\"           # root folder\n","\n","src_yaml = os.path.join(base_dir, name, 'data.yaml')\n","dst_yaml = os.path.join(out_dir, 'data.yaml')\n","\n","if make_yaml:\n","    new_class_ids = build_new_class_ids_from_yaml(\n","        src_yaml=src_yaml,\n","        allowed_ids=allowed_ids,\n","        collapse_map=collapse_map,\n","    )\n","    yaml_path = make_data_yaml(\n","        dataset_root=out_dir,\n","        new_class_ids=new_class_ids,\n","        has_test=None,  # auto-detect from folder existence\n","    )\n","    print('‚úÖ data.yaml written to:', yaml_path)\n","elif copy_yaml:\n","    if os.path.exists(src_yaml):\n","        shutil.copy2(src_yaml, dst_yaml)\n","        print(f'‚úÖ Copied data.yaml from {src_yaml} ‚Üí {dst_yaml}')\n","    else:\n","        print(f'‚ùå Source data.yaml not found at {src_yaml}')\n"]},{"cell_type":"markdown","source":["### Optional: Create a zip folder with the new dataset to dowload it"],"metadata":{"id":"ct9qNzOFPHz7"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"RKHvpSXMHJBZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761139329077,"user_tz":-120,"elapsed":1109,"user":{"displayName":"Angela Albi","userId":"15664309201935832859"}},"outputId":"538e33b7-b8ba-4266-c3cb-b526928a3969"},"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Zipped: /content/datasets/hexbugs-filtered_split.zip\n"]}],"source":["make_zip = True                 # set to False to skip zipping\n","\n","# === PATHS ===\n","folder = os.path.join(base_dir, f\"{name}-filtered_split\")\n","zip_path = os.path.join(base_dir, f\"{name}-filtered_split.zip\")\n","\n","# === CONDITIONAL ZIP ===\n","if make_zip:\n","    if os.path.exists(folder):\n","        if not os.path.exists(zip_path):\n","            !zip -r -q \"{zip_path}\" \"{folder}\"\n","            print(f\"‚úÖ Zipped: {zip_path}\")\n","        else:\n","            print(f\"‚ö†Ô∏è Zip file already exists: {zip_path}\")\n","    else:\n","        print(f\"‚ùå Folder not found: {folder}\")\n","else:\n","    print(\"‚è≠Ô∏è Skipping ZIP creation (make_zip=False)\")"]},{"cell_type":"markdown","metadata":{"id":"gAa5KIz7Cik3"},"source":["### Define output path"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BMy-FLWVwIG6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761139330418,"user_tz":-120,"elapsed":9,"user":{"displayName":"Angela Albi","userId":"15664309201935832859"}},"outputId":"0f7dc667-9c26-47fc-c071-defe42f18fe5"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/datasets\n","Directory '/content/drive/MyDrive/models/hexbugs' already exists.\n"]}],"source":["### Change path to your folder\n","REMOTE_URL = \"/content/drive/MyDrive/models/\" + name\n","HOME = \"/content/datasets/\"\n","\n","# Change to HOME directory\n","%cd {HOME}\n","\n","# Import os and create the folder if it doesn't exist\n","import os\n","\n","if not os.path.exists(REMOTE_URL):\n","    os.makedirs(REMOTE_URL)\n","    print(f\"Directory '{REMOTE_URL}' created.\")\n","else:\n","    print(f\"Directory '{REMOTE_URL}' already exists.\")"]},{"cell_type":"markdown","source":["# Training: Parameters"],"metadata":{"id":"dM5hr5hfPeV9"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"6xy69ImzmpvF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761139337485,"user_tz":-120,"elapsed":15,"user":{"displayName":"Angela Albi","userId":"15664309201935832859"}},"outputId":"133c40f9-ba03-4027-da44-331e4bb882a9"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/datasets\n","üîß Training params: translate=0.05 mixup=0.1 copy_paste=0.3 scale=0.3 mosaic=1 close_mosaic=10 line_width=1 nms=True plots=True cache=disk single_cls=False amp=True augment=True workers=16 multi_scale=True hsv_h=0.015 hsv_s=0.5 hsv_v=0.4 degrees=0 flipud=0.0 fliplr=0.0\n","üß™ resolution=320 | project=320-yolo11s-seg-mosaic | date_string=2025-10-22-13_hexbugs-1\n","üì¶ model=yolo11s-seg | base_model=yolo11s-seg | task=segment\n"]}],"source":["# Change to home directory\n","%cd {HOME}\n","\n","# ---- User-defined Settings ----\n","resolution = 1080                # Image resolution for training\n","epochs = 100                     # Number of training epochs\n","batch_size = 4                   # Batch size\n","base_model = \"yolo11s\"         # Choose model variant. Options: \"yolo11n-pose\", \"yolo11n-seg\", \"yolo11n\" etc.\n","\n","\n","# ---- Auto-detect task type ----\n","if \"-seg\" in base_model:\n","    task = \"segment\"\n","elif \"-pose\" in base_model:\n","    task = \"pose\"\n","else:\n","    task = \"detect\"\n","\n","# ---- üîß Training Settings ----\n","common_settings = {\n","    \"translate\": 0.05,       # Maximum image translation as data augmentation (in % of image size)\n","    \"mixup\": 0.1,          # MixUp blending factor for image mixing (usually low for object detection)\n","    \"copy_paste\": 0.3,      # Probability of using Copy-Paste augmentation (object pasting)\n","    \"scale\": 0.3,           # Random scaling of images for augmentation\n","    \"mosaic\": 1,             # Enable Mosaic augmentation (combines 4 images into 1)\n","    \"close_mosaic\": 10,      # Number of epochs before disabling mosaic for better fine-tuning\n","    \"line_width\": 1,         # Line width for label visualization\n","    \"nms\": True,             # Apply Non-Maximum Suppression during inference\n","    \"plots\": True,           # Save training plots (loss, mAP, etc.)\n","    \"cache\": \"disk\",         # Caching mode: \"disk\" to speed up I/O\n","    \"single_cls\": False,     # If True, treat all objects as one class (for class-agnostic detection)\n","    \"amp\": True,             # Enable automatic mixed precision (reduces memory, speeds up training)\n","    \"augment\": True,        # If True, applies augmentation at inference time\n","    \"workers\": 16,            # Number of dataloader workers (adjust depending on your CPU)\n","    \"multi_scale\": True,\n","    \"hsv_h\": 0.015,\n","    \"hsv_s\": 0.5,\n","    \"hsv_v\": 0.4\n","\n","}\n","\n","\n","# Modify task-specific augmentations\n","if task == \"detect\":\n","    common_settings.update({\n","        \"degrees\": 10,       # Allow full rotation\n","        \"flipud\": 0.0,       # Vertical flip probability\n","        \"fliplr\": 0.0        # Horizontal flip probability\n","    })\n","else:\n","    common_settings.update({\n","        \"degrees\": 0,         # No rotation for pose/seg\n","        \"flipud\": 0.0,\n","        \"fliplr\": 0.0\n","    })\n","\n","# Print CLI training parameters\n","parms = \" \".join([f\"{k}={v}\" for k, v in common_settings.items()])\n","print(\"üîß Training params:\", parms)\n","\n","# ---- üóÇÔ∏è Model Output Naming ----\n","from datetime import datetime\n","now = datetime.now()\n","date_string = now.strftime(\"%Y-%m-%d-%H\") + \"_\" + dataset.name.replace(\" \", \"-\") + \"-\" + str(dataset.version)\n","\n","project = f\"{resolution}-{base_model}\"\n","if common_settings[\"mosaic\"] > 0:\n","    project += \"-mosaic\"\n","\n","# if sharkcam:\n","#     project += \"-sharkcam\"  # Add logic if needed\n","\n","# ---- üß† Model Weights Source ----\n","model = base_model  # or path to a pretrained model\n","print(f\"üß™ resolution={resolution} | project={project} | date_string={date_string}\")\n","print(f\"üì¶ model={model} | base_model={base_model} | task={task}\")\n","\n","# ---- üîí Safety Check ----\n","import os\n","assert model == base_model or os.path.exists(model + \".pt\"), f\"Model path not found: {model}.pt\""]},{"cell_type":"markdown","metadata":{"id":"YUjFBKKqXa-u"},"source":["# Training: Run Command"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D2YkphuiaE7_","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1761139399233,"user_tz":-120,"elapsed":60247,"user":{"displayName":"Angela Albi","userId":"15664309201935832859"}},"outputId":"5012e1e5-f30b-481f-df9a-c34f2fae9d62"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/datasets\n","\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11s-seg.pt to 'yolo11s-seg.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 19.7MB 145.5MB/s 0.1s\n","Ultralytics 8.3.219 üöÄ Python-3.12.12 torch-2.8.0+cu126 CUDA:0 (NVIDIA A100-SXM4-40GB, 40507MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=True, auto_augment=randaugment, batch=24, bgr=0.0, box=7.5, cache=disk, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.3, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/datasets/hexbugs-filtered_split/data.yaml, degrees=0, deterministic=True, device=0, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=100, erasing=0.4, exist_ok=False, fliplr=0.0, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.5, hsv_v=0.4, imgsz=320, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=1, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.1, mode=train, model=yolo11s-seg.pt, momentum=0.937, mosaic=1, multi_scale=True, name=2025-10-22-13_hexbugs-1, nbs=64, nms=True, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=0, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=320-yolo11s-seg-mosaic, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/datasets/320-yolo11s-seg-mosaic/2025-10-22-13_hexbugs-1, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.3, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=segment, time=None, tracker=botsort.yaml, translate=0.05, val=True, verbose=True, vid_stride=1, visualize=True, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=16, workspace=None\n","\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 755.1KB 15.7MB/s 0.0s\n","Overriding model.yaml nc=80 with nc=1\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       928  ultralytics.nn.modules.conv.Conv             [3, 32, 3, 2]                 \n","  1                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n","  2                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  3                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  4                  -1  1    103360  ultralytics.nn.modules.block.C3k2            [128, 256, 1, False, 0.25]    \n","  5                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n","  6                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  7                  -1  1   1180672  ultralytics.nn.modules.conv.Conv             [256, 512, 3, 2]              \n","  8                  -1  1   1380352  ultralytics.nn.modules.block.C3k2            [512, 512, 1, True]           \n","  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n"," 10                  -1  1    990976  ultralytics.nn.modules.block.C2PSA           [512, 512, 1]                 \n"," 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 13                  -1  1    443776  ultralytics.nn.modules.block.C3k2            [768, 256, 1, False]          \n"," 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 16                  -1  1    127680  ultralytics.nn.modules.block.C3k2            [512, 128, 1, False]          \n"," 17                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 19                  -1  1    345472  ultralytics.nn.modules.block.C3k2            [384, 256, 1, False]          \n"," 20                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n"," 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 22                  -1  1   1511424  ultralytics.nn.modules.block.C3k2            [768, 512, 1, True]           \n"," 23        [16, 19, 22]  1   1474291  ultralytics.nn.modules.head.Segment          [1, 32, 128, [128, 256, 512]] \n","YOLO11s-seg summary: 203 layers, 10,082,675 parameters, 10,082,659 gradients, 33.1 GFLOPs\n","\n","Transferred 555/561 items from pretrained weights\n","Freezing layer 'model.23.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 5.4MB 68.7MB/s 0.1s\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 3284.0¬±630.4 MB/s, size: 521.1 KB)\n","\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/datasets/hexbugs-filtered_split/train/labels... 25 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 25/25 1.2Kit/s 0.0s\n","\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/datasets/hexbugs-filtered_split/train/labels.cache\n","\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.6GB Disk): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 25/25 91.3it/s 0.3s\n","\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n","\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1319.6¬±846.0 MB/s, size: 529.6 KB)\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/datasets/hexbugs-filtered_split/valid/labels... 7 images, 0 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7/7 883.6it/s 0.0s\n","\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/datasets/hexbugs-filtered_split/valid/labels.cache\n","\u001b[K\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.2GB Disk): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 7/7 45.8it/s 0.2s\n","Plotting labels to /content/datasets/320-yolo11s-seg-mosaic/2025-10-22-13_hexbugs-1/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 90 weight(decay=0.0), 101 weight(decay=0.0005625000000000001), 100 bias(decay=0.0)\n","Image sizes 320 train, 320 val\n","Using 12 dataloader workers\n","Logging results to \u001b[1m/content/datasets/320-yolo11s-seg-mosaic/2025-10-22-13_hexbugs-1\u001b[0m\n","Starting training for 100 epochs...\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      1/100      3.18G      1.515      6.503      2.945     0.8784          8        288: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2/2 0.1it/s 20.8s\n","\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 1/1 0.3it/s 3.3s\n","                   all          7         35      0.305      0.229      0.173      0.119      0.325      0.234      0.242      0.124\n","\n","      Epoch    GPU_mem   box_loss   seg_loss   cls_loss   dfl_loss  Instances       Size\n","\u001b[K      2/100      3.19G      3.207      5.688        3.7       1.02        182        160: 0% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0/2  5.0s^C\n"]}],"source":["    # Change to your working directory\n","%cd {HOME}\n","\n","# ---- Launch YOLO training ----\n","yolo_cmd = f\"\"\"\n","yolo task={task} \\\n","     mode=train \\\n","     resume=False \\\n","     model={model}.pt \\\n","     data={out_dir}/data.yaml \\\n","     device=0 \\\n","     name={date_string} \\\n","     project={project} \\\n","     epochs={epochs} \\\n","     imgsz={resolution} \\\n","     batch={batch_size} \\\n","     patience=0 \\\n","     visualize=True \\\n","     {parms}\n","\"\"\"\n","\n","# ‚ñ∂Ô∏è Run the command\n","!{yolo_cmd}"]},{"cell_type":"markdown","metadata":{"id":"6BMmQ2P0p6gH"},"source":["#5) Locate last trained model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IIwmALvQnStL"},"outputs":[],"source":["# Change to the working directory\n","%cd {HOME}\n","\n","# List all subdirectories in the project folder\n","all_subdirs = [project + '/' + d for d in os.listdir(project)]\n","\n","# Keep only those that contain a trained model\n","all_subdirs = [d for d in all_subdirs if os.path.exists(d + \"/weights/last.pt\")]\n","\n","# Get the most recently modified subdirectory\n","latest_subdir = max(all_subdirs, key=os.path.getmtime)\n","\n","# Construct the full path to the latest run\n","full_path = HOME + \"/\" + latest_subdir\n","\n","print(project)\n","print(latest_subdir)\n","print(full_path)\n","\n","# Save training parameters to a parms.txt\n","!echo \"{parms}\" > {latest_subdir}/parms.txt"]},{"cell_type":"markdown","metadata":{"id":"iq8q97TnoXop"},"source":["### Select and Save Best YOLO Model Based on mAP Metrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWRI444BniY1"},"outputs":[],"source":["# Check if the best model weights file exists\n","print(os.path.exists(latest_subdir + \"/weights/best.pt\"))\n","\n","# Load training results CSV\n","import pandas as pd\n","csv = pd.read_csv(latest_subdir + \"/results.csv\")\n","\n","# Strip whitespace from column names\n","for c in csv.columns:\n","    csv = csv.rename(columns={c: c.strip()})\n","    # print(c.strip())  # Optional: print cleaned column names\n","\n","# Check if metrics for both M (mask) and B (box) exist, and compute a weighted average\n","if \"metrics/mAP50-95(M)\" in csv.columns:\n","    # Combined score: weighted average of mask and box metrics (90% mAP50-95 + 10% mAP50)\n","    combined = (csv[\"metrics/mAP50-95(M)\"] * 0.9 + csv[\"metrics/mAP50(M)\"] * 0.1) + \\\n","               (csv[\"metrics/mAP50-95(B)\"] * 0.9 + csv[\"metrics/mAP50(B)\"] * 0.1)\n","\n","    # Get index of best epoch based on combined score\n","    index = combined.argmax()\n","\n","    # Extract best mAP values for masks\n","    best_map50_95 = csv[\"metrics/mAP50-95(M)\"].values[index]\n","    best_map50 = csv[\"metrics/mAP50(M)\"].values[index]\n","else:\n","    # Only box metrics available; compute weighted score accordingly\n","    combined = (csv[\"metrics/mAP50-95(B)\"] * 0.9 + csv[\"metrics/mAP50(B)\"] * 0.1)\n","    index = combined.argmax()\n","\n","    # Extract best mAP values for boxes\n","    best_map50_95 = csv[\"metrics/mAP50-95(B)\"].values[index]\n","    best_map50 = csv[\"metrics/mAP50(B)\"].values[index]\n","\n","# Define source path of best model\n","from_path = latest_subdir + \"/weights/best.pt\"\n","\n","# Define destination path with project name, date, and mAP scores in filename\n","to_path = HOME + \"/\" + project + \"-\" + date_string + \"-mAP5095_\" + str(best_map50_95) + \"-mAP50_\" + str(best_map50) + \".pt\"\n","to_path = \"/content/\" + project + \"-\" + date_string + \"-mAP5095_\" + str(best_map50_95) + \"-mAP50_\" + str(best_map50) + \".pt\"\n","\n","# Log the copy action with source and destination paths\n","print(\"copying from \", from_path, \"to\", to_path)\n","\n","# Copy the best model weights to the destination path with informative filename\n","!cp {from_path} {to_path}\n","\n","# Upload the copied model file to a remote location using rsync with progress display\n","!rsync --progress {to_path} {REMOTE_URL}/\n","\n","# Create a ZIP archive of the full training results folder\n","!zip -r \"{HOME}/{latest_subdir}.zip\" \"{full_path}\"\n","\n","# Upload the zipped training results to the remote server using rsync with progress shown\n","!rsync --progress \"{HOME}/{latest_subdir}.zip\" \"{REMOTE_URL}/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wFKyygZYZsaW"},"outputs":[],"source":["!rsync --progress {to_path} {REMOTE_URL}/\n"]},{"cell_type":"markdown","metadata":{"id":"sbj_y1WCpI0q"},"source":["### Training results plot"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-urTWUkhRmn"},"outputs":[],"source":["# Change working directory to HOME\n","%cd {HOME}\n","\n","# Display the training results plot (e.g. loss and metrics curves)\n","Image(filename=f'{latest_subdir}/results.png', width=1200)"]},{"cell_type":"markdown","metadata":{"id":"zjYMah2GpMo4"},"source":["### Sample batch of validation predictions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HI4nADCCj3F5"},"outputs":[],"source":["# Change working directory to HOME\n","%cd {HOME}\n","\n","# Display a sample batch of validation predictions (visual output of model)\n","Image(filename=f'{latest_subdir}/val_batch0_pred.jpg', width=600)"]},{"cell_type":"markdown","metadata":{"id":"6ODk1VTlevxn"},"source":["# 6) Validate Custom Model\n","\n","This step runs **model validation** using the best trained checkpoint (`best.pt`) on the validation dataset defined in `data.yaml`. It evaluates the model's performance using standard YOLO metrics, such as:\n","\n","- **mAP50**: mean Average Precision at IoU threshold 0.5\n","- **mAP50-95**: mean AP across IoU thresholds from 0.5 to 0.95\n","- **Precision & Recall** for each class\n","\n","The validation results will be saved inside the specified project folder and include:\n","\n","- A `results.png` file with training/validation curves\n","- A `confusion_matrix.png` for classification performance\n","- A `val_batch0_pred.jpg` showing predicted bounding boxes on a sample batch\n","\n","You can use these visual and quantitative outputs to assess if the model generalizes well to unseen data. [link text](https://)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YpyuwrNlXc1P"},"outputs":[],"source":["# Change working directory to HOME\n","%cd {HOME}\n","\n","# Run YOLO validation on the best model checkpoint using the specified dataset and image size\n","!yolo task={task} mode=val model={latest_subdir}/weights/best.pt data={dataset.location}/data.yaml project={project} imgsz={resolution} line_width=1"]},{"cell_type":"markdown","metadata":{"id":"i4eASbcWkQBq"},"source":["# 7) Run Inference on Validation Images\n","\n","This step performs **inference (prediction)** using the best trained YOLO model (`best.pt`) on the validation image set. It is useful to **visually inspect how the model performs** on real images after training.\n","\n","What this does:\n","\n","- Removes any existing `predict` folder to avoid clutter or overwriting previous predictions\n","- Runs YOLO in `predict` mode using:\n","  - The best model checkpoint\n","  - Images from the validation set\n","  - A low confidence threshold (`conf=0.1`) to allow more predictions for visual inspection\n","  - The specified image size (`imgsz`)\n","- Saves predicted images (with boxes, masks, or keypoints depending on the task) in a new folder under the project directory: `runs/predict`\n","\n","This is especially helpful for qualitatively checking the model's detection performance, spotting failure cases, or selecting images for visualization or presentations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wjc1ctZykYuf"},"outputs":[],"source":["# Change working directory to HOME\n","%cd {HOME}\n","\n","# Remove any previous YOLO prediction results to avoid overwriting conflicts\n","%rm -rf {latest_subdir}/../predict\n","\n","# Run YOLO prediction on validation images using the best model checkpoint\n","!yolo task={task} mode=predict model={latest_subdir}/weights/best.pt project={project} name=predict conf=0.1 source={dataset.location}/valid/images save=true imgsz={resolution} line_width=1"]},{"cell_type":"markdown","metadata":{"id":"vksXiQIBp7pJ"},"source":["### Zip and Save Prediction Results\n","\n","This step creates a ZIP archive of the prediction results generated in the previous step. The archive is saved in your home directory and named using the training subdirectory name (to make it easy to track which model it came from).\n","\n","This makes it simple to download, share, or upload the predictions for external use (e.g., for presentations, manual inspection, or further analysis)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSrUCvQS7wpK"},"outputs":[],"source":["# Create a ZIP archive of the YOLO prediction results\n","# The ZIP file will be named using the current training subdirectory name to keep it traceable\n","zipname = latest_subdir.replace('/', '_')\n","!zip -r \"{HOME}/prediction_{zipname}.zip\" {HOME}/{project}/predict -i \"{HOME}/{project}/predict/*\"\n","\n","# Upload the zipped prediction results to the remote server using rsync with progress feedback\n","!rsync --progress \"{HOME}/prediction_{zipname}.zip\" \"{REMOTE_URL}/\""]},{"cell_type":"markdown","metadata":{"id":"0znAqJQw77sc"},"source":["# 10) Display Sample Predictions\n","\n","This step randomly selects and displays 5 predicted images from the `predict` folder.\n","\n","Each image includes the model's output (e.g., bounding boxes, masks, or keypoints) overlaid on the validation images.  \n","It provides a quick **visual inspection** of model performance across different examples.  \n","\n","This qualitative check helps identify:\n","- How well the model localizes objects\n","- Possible false positives or negatives\n","- Class confusion or missed detections"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jbVjEtPAkz3j"},"outputs":[],"source":["# Randomly select 5 predicted images from the YOLO prediction output folder\n","files = np.random.choice(glob.glob(f'{HOME}/{project}/predict/*.jpg'), size=5)\n","print(files.shape)\n","\n","# Display each selected image and print a newline for spacing\n","for image_path in files:\n","    display(Image(filename=image_path, height=600))\n","    print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"V4x6ansX2Yyd"},"outputs":[],"source":["\n","\n","# Wait for 30 seconds (e.g., to ensure all background tasks finish before disconnecting)\n","time.sleep(30)\n","\n","# Gracefully disconnect the current Colab runtime session\n","runtime.unassign()\n"]},{"cell_type":"markdown","metadata":{"id":"ovQgOj_xSNDg"},"source":["## üèÜ Congratulations\n","\n","### Find more learning resources here\n","\n","Roboflow has produced many resources that you may find interesting as you advance your knowledge of computer vision:\n","\n","- [Roboflow Notebooks](https://github.com/roboflow/notebooks): A repository of over 20 notebooks that walk through how to train custom models with a range of model types, from YOLOv7 to SegFormer.\n","- [Roboflow YouTube](https://www.youtube.com/c/Roboflow): Our library of videos featuring deep dives into the latest in computer vision, detailed tutorials that accompany our notebooks, and more.\n","- [Roboflow Discuss](https://discuss.roboflow.com/): Have a question about how to do something on Roboflow? Ask your question on our discussion forum.\n","- [Roboflow Models](https://roboflow.com): Learn about state-of-the-art models and their performance. Find links and tutorials to guide your learning.\n","\n","### Convert data formats\n","\n","Roboflow provides free utilities to convert data between dozens of popular computer vision formats. Check out [Roboflow Formats](https://roboflow.com/formats) to find tutorials on how to convert data between formats in a few clicks.\n","\n","### Connect computer vision to your project logic\n","\n","[Roboflow Templates](https://roboflow.com/templates) is a public gallery of code snippets that you can use to connect computer vision to your project logic. Code snippets range from sending emails after inference to measuring object distance between detections."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BDXH0Pin6_p1"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"A100","provenance":[{"file_id":"1xA7V48NoAac9KIDTxZmTAceNfLI-_grs","timestamp":1760991558640},{"file_id":"1pjtDbA7W4bGm6AilfSAsp7umcpQqug0E","timestamp":1753455882577},{"file_id":"1mgATEXF9Q3uwyqn36zARJuN-SCao0vWY","timestamp":1749540423452},{"file_id":"176M7maMiXoBdwzx56eTL-vUzqoZnhWQ7","timestamp":1743949490546},{"file_id":"1AySwaqzLY_9pbYcCseDNzSP5LfPFEn1N","timestamp":1739984495010},{"file_id":"11bWpJfKdyL0TPZEoEn1aRNVcu6VqfJXW","timestamp":1739386893949},{"file_id":"12DfvYYsCXJyWvcuLRC3-repiqUu-j-lo","timestamp":1738072102162},{"file_id":"1Ix0ZQUG-yC-M668XAatz6diTukf82Fj-","timestamp":1728593661041},{"file_id":"1mr0_EF8hD0Ysek8Ut29cSDT8e51fJtpN","timestamp":1697065971552},{"file_id":"1qXAoEalGmD2MEd4Uf3RXDHWgPQ56W4Gb","timestamp":1689629991194},{"file_id":"11DBtUd-3b7Imak3MGlRlLv8Bf1z58zOF","timestamp":1686323749856},{"file_id":"1y0znqq-puMHI74ibJg9OpUYwBT-DmBu5","timestamp":1686266012804},{"file_id":"1c4fQ7ireKC3Foh_zi4pnb0EOtLDKBEk_","timestamp":1685190120096},{"file_id":"https://github.com/roboflow-ai/notebooks/blob/main/notebooks/train-yolov8-instance-segmentation-on-custom-dataset.ipynb","timestamp":1685016280455}]},"kernelspec":{"display_name":"data_analysis_py310","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.18"},"widgets":{"application/vnd.jupyter.widget-state+json":{"d17beb7197e343719c1dd849c8bccf9d":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_5f86d560cb3b4e73aa4ba34ae62dfe29","IPY_MODEL_4ef1d6b0299445b79c99bcd9b60da725","IPY_MODEL_8f782975d4ff4c759bc6377ac3d51b2b","IPY_MODEL_974842e0ad2c474d8815776f27aae193","IPY_MODEL_6cf06b586d6445989afe76aa0f8e6629","IPY_MODEL_46c3983db70e481b9798000a583e5fc6"],"layout":"IPY_MODEL_a8db64c0155f4a1a98175d0ab7a7cbab"}},"5f86d560cb3b4e73aa4ba34ae62dfe29":{"model_module":"@jupyter-widgets/controls","model_name":"DropdownModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DropdownModel","_options_labels":["Roboflow snippet","Google Drive link","Example dataset"],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"DropdownView","description":"Source:","description_tooltip":null,"disabled":false,"index":0,"layout":"IPY_MODEL_6ac4fe28b6b544cb8c69dcd6f4110a67","style":"IPY_MODEL_05c134ced70444899ce786ec7f2c1505"}},"4ef1d6b0299445b79c99bcd9b60da725":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_70445951fbaa49a38fc1b7c3515e8cd1","IPY_MODEL_f28898bf3fd940d785ccc2f3cec492cf"],"layout":"IPY_MODEL_cea5eb5ca4ed44568e9f6027994bccc9"}},"8f782975d4ff4c759bc6377ac3d51b2b":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"VBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"VBoxView","box_style":"","children":["IPY_MODEL_5cc9056bcf594ccb8b2e28e44e872627","IPY_MODEL_5bfe88b3871d4fe4a6e2f8fd5a071b0d","IPY_MODEL_79861276c5b340289c2d9a20f85bda15"],"layout":"IPY_MODEL_aef7e6cb6c2a4e9ba33008767485870e"}},"974842e0ad2c474d8815776f27aae193":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_86368bf2e52a479bb18fd5f0b668e0d3","placeholder":"‚Äã","style":"IPY_MODEL_1367db7f2eb44dba9de71a5b004085ec","value":"The example dataset will be downloaded from <code>albiangela/TREx-tutorials-data</code>."}},"6cf06b586d6445989afe76aa0f8e6629":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"primary","description":"Download dataset","disabled":false,"icon":"download","layout":"IPY_MODEL_8dbfe0d25cfc45478b21d391ef42dd34","style":"IPY_MODEL_7311b0c2bffc4ba485716857b97041e8","tooltip":""}},"46c3983db70e481b9798000a583e5fc6":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_9424484fef9b441f8c99c62a5487c42b","msg_id":"","outputs":[]}},"a8db64c0155f4a1a98175d0ab7a7cbab":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ac4fe28b6b544cb8c69dcd6f4110a67":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05c134ced70444899ce786ec7f2c1505":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"120px"}},"70445951fbaa49a38fc1b7c3515e8cd1":{"model_module":"@jupyter-widgets/controls","model_name":"TextareaModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextareaModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextareaView","continuous_update":true,"description":"Snippet:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_107b3677b0d0480a96d367f892a70ab2","placeholder":"Paste the full Roboflow download snippet here‚Ä¶","rows":null,"style":"IPY_MODEL_49f1c1507e194fa587a3359a3eb878d1","value":""}},"f28898bf3fd940d785ccc2f3cec492cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6788544e0fdd4ad7b51f0fe6770622f9","placeholder":"‚Äã","style":"IPY_MODEL_883074ceb2af46428b058128e2ba9346","value":"<small>Tip: Paste the lines that include <code>api_key</code>, <code>workspace()</code>, <code>project()</code>, <code>version()</code>, and <code>download()</code>.</small>"}},"cea5eb5ca4ed44568e9f6027994bccc9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"flex","flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5cc9056bcf594ccb8b2e28e44e872627":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"Link:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_bd462e3e0a5b410d9a72ea520ba7f6fc","placeholder":"https://drive.google.com/file/d/‚Ä¶","style":"IPY_MODEL_e544351928b6479ea28f976f52b819f3","value":""}},"5bfe88b3871d4fe4a6e2f8fd5a071b0d":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"TextModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"TextView","continuous_update":true,"description":"Dataset name:","description_tooltip":null,"disabled":false,"layout":"IPY_MODEL_bd462e3e0a5b410d9a72ea520ba7f6fc","placeholder":"Optional folder name","style":"IPY_MODEL_063d9dc5ec064636bbd762d374c100b9","value":""}},"79861276c5b340289c2d9a20f85bda15":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d806564cd6b648e9b36711267e96c353","placeholder":"‚Äã","style":"IPY_MODEL_89aaf2288271422699438d019dea1dab","value":"<small>Accepts a full shareable link or just the file id. Leave blank to keep the zip name.</small>"}},"aef7e6cb6c2a4e9ba33008767485870e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"none","flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86368bf2e52a479bb18fd5f0b668e0d3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":"none","flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1367db7f2eb44dba9de71a5b004085ec":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8dbfe0d25cfc45478b21d391ef42dd34":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7311b0c2bffc4ba485716857b97041e8":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"107b3677b0d0480a96d367f892a70ab2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":"220px","justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"49f1c1507e194fa587a3359a3eb878d1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"120px"}},"6788544e0fdd4ad7b51f0fe6770622f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"883074ceb2af46428b058128e2ba9346":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bd462e3e0a5b410d9a72ea520ba7f6fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"100%"}},"e544351928b6479ea28f976f52b819f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"120px"}},"063d9dc5ec064636bbd762d374c100b9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":"120px"}},"d806564cd6b648e9b36711267e96c353":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89aaf2288271422699438d019dea1dab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9424484fef9b441f8c99c62a5487c42b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}